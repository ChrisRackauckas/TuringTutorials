{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probabilistic Programming with Turing\n",
    "\n",
    "---\n",
    "\n",
    "## 0. Introduction\n",
    "This notebook is the first of a series of tutorials on the universal probabilistic programming language **Turing**.\n",
    "\n",
    "**Turing** is probabilistic programming system written entirely in *Julia*. It has an intuitive modelling syntax and supports a wide range of sampling-based inference algorithms. Most importantly, **Turing** inference is composable: it combines Markov chain sampling operations on subsets of model variables, e.g. using a combination of a Hamiltonian Monte Carlo (HMC) engine and a particle Gibbs (PG) engine. This composable inference engine allows the user to easily switch between black-box style inference methods such as HMC and customized inference methods.  \n",
    " \n",
    "---\n",
    "Familiarity with Julia is assumed through out this notebook. If you are new to Julia, [Learning Julia](https://julialang.org/learning/) is a good starting point.\n",
    "\n",
    "For users new to Bayesian machine learning we refer to further resources, e.g. the Pattern Recognition and Machine Learning book. This notebook tries to provide an intuition for Bayesian inference and gives a simple example on how to use **Turing**. Note that this notebook is not a comprehensive introduction to Bayesian machine learning. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Coin-Flipping  (Julia 1.0)\n",
    "The following example aims to illustrate the effect of updating our beliefs with each new evidence we observe. In particular, we will assume that we are unsure about the probability of heads in a coin flip. To get an intuitive understanding of what \"updating our beliefs\" is, we will visualize the probability of heads in a coin flip after each observed evidence.\n",
    "\n",
    "---\n",
    "Note that we will not need **Turing** for this particular example. If you are familiar with posterior updates, feel free to proceed to the next tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using Base modules\n",
    "using Random\n",
    "\n",
    "# load a plotting library\n",
    "using Plots\n",
    "\n",
    "# use the PyPlot backend for plotting (this is optional)\n",
    "#pyplot();\n",
    "\n",
    "# load the distributions library\n",
    "using Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the true probability of heads in a coin\n",
    "p_true = 0.5\n",
    "\n",
    "# iterate from having seen 0 observations to 100 observations\n",
    "Ns = 0:100\n",
    "\n",
    "# draw data from a Bernoulli distribution, i.e. draw heads or tails\n",
    "Random.seed!(12)\n",
    "data = rand(Bernoulli(p_true), last(Ns))\n",
    "\n",
    "# our prior belief about the probability of heads in a coin\n",
    "prior_belief = Beta(1, 1)\n",
    "\n",
    "# this is required for plotting only\n",
    "x = range(0, stop = 1, length = 100)\n",
    "\n",
    "# make an animation\n",
    "animation = @animate for (i, N) in enumerate(Ns)\n",
    "\n",
    "    # count the number of heads\n",
    "    heads = sum(data[1:i-1])\n",
    "    \n",
    "    # update our prior belief in closed form (this is possible because we use a conjugate prior)\n",
    "    updated_belief = Beta(prior_belief.α + heads, prior_belief.β + N - heads)\n",
    "\n",
    "    # plotting\n",
    "    plot(x, pdf.(Ref(updated_belief), x), \n",
    "        size = (500, 250), \n",
    "        title = \"Updated belief after $N observations\",\n",
    "        xlabel = \"probability of heads\", \n",
    "        ylabel = \"\", \n",
    "        legend = nothing,\n",
    "        xlim = (0,1),\n",
    "        fill=0, α=0.3, w=3)\n",
    "    vline!([p_true])\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![animation](https://user-images.githubusercontent.com/7974003/44995702-37c1b200-af9c-11e8-8b26-c88a528956af.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The animation above shows that with increasing evidence our belief about the probability of heads in a coin flip turns towards the true value (compare the mode of the beta distribution to the orange line). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Coin-Flipping - part 2  (Julia 1.0)\n",
    "\n",
    "In the previous example, we used the fact that our prior distribution is a conjugate prior. Note that a closed-form expression for the posterior is not accessible in general and usually does not exist for more interesting models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load Turing and MCMCChain\n",
    "using Turing, MCMCChain\n",
    "\n",
    "# load stats plots for density plots\n",
    "using StatPlots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following, we will compare the result of Turing against the posterior distribution obtained in closed-form.\n",
    "\n",
    "---\n",
    "\n",
    "At first, we will define the coin-flip model using Turing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@model coinflip(y) = begin\n",
    "    \n",
    "    # our prior belief about the probability of heads in a coin \n",
    "    p ~ Beta(1, 1)\n",
    "    \n",
    "    # the number of observations\n",
    "    N = length(y)\n",
    "    for n in 1:N\n",
    "        # heads or tails of a coin are drawn from a Bernoulli distribution\n",
    "        y[n] ~ Bernoulli(p)\n",
    "    end\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After defining the model, we can approximate the posterior distribution using samples from the distribution. In this example, we use a Hamiltonian Monte Carlo sampler to construct these samples. Later tutorials will give more information on the samplers available in Turing and discuss their use for different models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting of Hamiltonian Monte Carlo (HMC) sampler\n",
    "iterations = 1000\n",
    "ϵ = 0.05\n",
    "τ = 10\n",
    "\n",
    "# start sampling\n",
    "chain = sample(coinflip(data), HMC(iterations, ϵ, τ));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "After finishing the sampling process, we can visualize the posterior distribution approximated using Turing against the posterior distribution in closed-form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct summary of the sampling process for the parameter p, i.e. the probability of heads in a coin\n",
    "p_summary = Chains(chain[:p])\n",
    "\n",
    "# compute the posterior distribution in closed-form \n",
    "N = length(data)\n",
    "heads = sum(data)\n",
    "updated_belief = Beta(prior_belief.α + heads, prior_belief.β + N - heads)\n",
    "\n",
    "# visualize a blue density plot of the approximate posterior distribution using HMC (see Chain 1 in the legend)\n",
    "p = densityplot(p_summary, xlim = (0,1), legend = :best, w = 2, c = :blue)\n",
    "\n",
    "# visualize a green density plot of posterior distribution in closed-form\n",
    "plot!(p, range(0, stop = 1, length = 100), pdf.(Ref(updated_belief), range(0, stop = 1, length = 100)), \n",
    "        xlabel = \"probability of heads\", ylabel = \"\", title = \"\", xlim = (0,1), label = \"closed-form\",\n",
    "        fill=0, α=0.3, w=3, c = :lightgreen)\n",
    "\n",
    "# visualize the true probability of heads in red\n",
    "vline!(p, [p_true], label = \"true probability\", c = :red)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![sdf](https://user-images.githubusercontent.com/7974003/44995682-25477880-af9c-11e8-850b-36e4b6d756ea.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.0.0",
   "language": "julia",
   "name": "julia-1.0"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
